{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data wrangling : sparsify, use indices for labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import sparse as sps\n",
    "import time\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = pd.read_csv('training_environments.csv', index_col=0)\n",
    "empo_names = [f'empo_{i}' for i in range(1, 4)]\n",
    "empo_index_to_label = []\n",
    "\n",
    "for empo in empo_names:\n",
    "    empo_index_to_label.append([str(row) for row in envs.drop_duplicates(subset=empo)[empo]])\n",
    "    \n",
    "empo_label_to_index = {name : {label : i for i, label in enumerate(labels)} for name, labels in zip(empo_names, empo_index_to_label)}\n",
    "empo_label_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace text labels with integers\n",
    "envs = envs.replace(empo_label_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_sparse(in_filename, out_filename):\n",
    "    line_count = sum(1 for line in open(in_filename))\n",
    "    rows = []\n",
    "    with open(in_filename) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            row = [int(x) for x in line.strip().split(',')[1:]]\n",
    "            row = sps.csr_matrix(row)\n",
    "            rows.append(row)\n",
    "\n",
    "            if i % 1000 == 0:\n",
    "                print(f'Sparsifying {in_filename} [row {i} / {line_count}]\\r')\n",
    "    mat = sps.vstack(rows)\n",
    "    \n",
    "    sps.save_npz(out_filename, mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def maybe_sparsify(in_filename, out_filename):\n",
    "    if not Path(out_filename).is_file():\n",
    "        save_as_sparse(in_filename, out_filename)\n",
    "        \n",
    "def get_header_line(csv_file):\n",
    "    with open(csv_file) as f:\n",
    "        line = next(f)\n",
    "        return np.array(line.rstrip().split(',')[1:])\n",
    "    \n",
    "maybe_sparsify('training_descriptors.csv', 'training_descriptors_sparse.npz')\n",
    "maybe_sparsify('challenge_descriptors.csv', 'challenge_descriptors_sparse.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_species_names = get_header_line('training_descriptors_header.csv')\n",
    "\n",
    "desc = sps.load_npz('training_descriptors_sparse.npz')\n",
    "species = pd.read_csv('bacterial_species.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_megabytes(a):\n",
    "    return (a.data.nbytes + a.indptr.nbytes + a.indices.nbytes) / (1024 * 1024)\n",
    "\n",
    "print(f'In-memory size of desc : {sparse_megabytes(desc):.2f}M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_taxonomy_df(desc, taxon_level):\n",
    "    taxons = species[taxon_level][desc_species_names]\n",
    "    columns = taxons.unique()\n",
    "\n",
    "    taxon_indices, taxon_names = pd.factorize(taxons)\n",
    "    \n",
    "    data = np.ones(taxon_indices.shape)\n",
    "    row_ind = np.arange(taxon_indices.shape[0])\n",
    "    col_ind = taxon_indices\n",
    "    \n",
    "    D = sps.csr_matrix((data, (row_ind, col_ind)))\n",
    "    \n",
    "    table = desc @ D\n",
    "    \n",
    "    return taxon_names, table\n",
    "\n",
    "taxon_names = {}\n",
    "taxons = {}\n",
    "\n",
    "for taxon_level in species.columns:\n",
    "    taxon_names[taxon_level], taxons[taxon_level] = to_taxonomy_df(desc, taxon_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold, cross_validate\n",
    "\n",
    "clf_logit = LogisticRegression(random_state=0, n_jobs=-1)\n",
    "clf_rforest = RandomForestClassifier(random_state=0, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_clf(clf, desc, empo, samples=None, fast=False, n_splits=5):\n",
    "    samples = samples if samples is not None else desc.shape[0]\n",
    "    \n",
    "    # shuffle and truncate data\n",
    "    idx = np.arange(samples)\n",
    "\n",
    "    gen = np.random.default_rng(0)\n",
    "    gen.shuffle(idx)\n",
    "    idx = idx[:samples]\n",
    "    \n",
    "    desc = desc[idx]\n",
    "    empo = empo[idx]\n",
    "    \n",
    "    if fast:\n",
    "        print('Warning : using fast evaluation, cross-validation turned off.')\n",
    "        \n",
    "        train_pcent = 1 - 1 / n_splits\n",
    "        train_count = int(round(train_pcent * samples))\n",
    "\n",
    "        desc_train = desc_shuf[:train_count]\n",
    "        desc_validate = desc_shuf[train_count:]\n",
    "\n",
    "        empo_train = empo[:train_count]\n",
    "        empo_validate = empo[train_count:]\n",
    "\n",
    "        clf.fit(desc_train, empo_train)\n",
    "        accuracy = clf.score(desc_validate, empo_validate)\n",
    "        accuracies = np.array([accuracy])\n",
    "        \n",
    "        f1 = f1_score(clf.predict(desc_validate), empo_validate, average='weighted')\n",
    "        f1 = np.array([f1])\n",
    "        \n",
    "        return {'test_accuracy' : accuracies, 'test_f1_weighted' : f1}\n",
    "    else:\n",
    "        # cross validation\n",
    "        k_folds = KFold(n_splits=n_splits, shuffle=True, random_state=0)\n",
    "    \n",
    "        return cross_validate(clf, desc, empo, cv=k_folds, scoring=['accuracy', 'f1_weighted'], n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### scores.csv file contains all the information about the cv scores, the computational time, the F1 score, using the different taxonomies and all the initial descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def write_scores_file():\n",
    "    scores = []\n",
    "    features = [(taxon_level, taxons[taxon_level]) for taxon_level in species.columns]\n",
    "    features.append(('desc', desc))\n",
    "\n",
    "    samples=None\n",
    "    for clf_name, clf in [('rforest', clf_rforest), ('logit', clf_logit)]:\n",
    "        for empo in empo_names:\n",
    "            for feature_name, feature_vector in features:\n",
    "                print(clf_name, empo, feature_name)\n",
    "\n",
    "                s = cross_validate_clf(clf, feature_vector, envs[empo], samples=samples)\n",
    "                s['clf_name'] = clf_name\n",
    "                s['empo'] = empo\n",
    "                s['features'] = feature_name\n",
    "\n",
    "                scores.append(s)\n",
    "\n",
    "    scores = pd.DataFrame(scores)\n",
    "    for k in ['test_accuracy', 'score_time', 'test_f1_weighted', 'fit_time']:\n",
    "        scores[k + '_median'] = scores[k].apply(lambda x : np.median(x))\n",
    "    scores.to_csv('scores.csv')\n",
    "    \n",
    "write_scores_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for taxonomy in species:\n",
    "    print(taxonomy, species[taxonomy].unique().size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def biological_interpretation(taxon_level, empo_name, n, out=sys.stdout):\n",
    "    feature_vector = taxons[taxon_level]\n",
    "    n_features = feature_vector.shape[-1]\n",
    "\n",
    "    clf_logit.fit(feature_vector, envs[empo_name])\n",
    "    \n",
    "    feature_order_idx = np.argsort(clf_logit.coef_, axis=1)\n",
    "    if len(empo_label_to_index[empo_name]) == 2:\n",
    "        feature_order_idx = np.array([feature_order_idx[0][::-1], feature_order_idx[0]])\n",
    "    \n",
    "    for env_name, env_idx in empo_label_to_index[empo_name].items():\n",
    "        print(f'{env_name} :', file=out)\n",
    "\n",
    "        bottom_n = taxon_names[taxon_level][feature_order_idx[env_idx][:n]]\n",
    "        top_n = taxon_names[taxon_level][feature_order_idx[env_idx][-n:]]\n",
    "        print(f'  Top {n} least correlated : {\", \".join(name[5:] for name in bottom_n)}', file=out)\n",
    "        print(f'  Top {n} most correlated : {\", \".join(name[5:] for name in top_n)}', file=out)\n",
    "\n",
    "for taxon_level in taxon_names:\n",
    "    if taxon_level != 'taxonomy_0':\n",
    "        with open(f'biological_interpretation/{taxon_level}.txt', 'w') as f:\n",
    "            for empo_name in empo_names:\n",
    "                print(f'{empo_name} :', file=f)\n",
    "                biological_interpretation(taxon_level, empo_name, 5, out=f)\n",
    "                print('', file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction Bacteria dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best number of components for Truncated SVD\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "explained_variance_ratio = []\n",
    "score_cv = []\n",
    "time_cv = []\n",
    "\n",
    "def eval_clf(clf, desc, empo_name):\n",
    "    scores = cross_validate_clf(clf, desc, empo_name)\n",
    "    return scores['test_accuracy'], scores['fit_time']\n",
    "\n",
    "for i in [100,200,500,1000,2000]:\n",
    "    print(i)\n",
    "    svd = TruncatedSVD(n_components=i)\n",
    "    svd.fit(desc)\n",
    "    desc_reduced = svd.transform(desc)\n",
    "    explained_variance_ratio.append(svd.explained_variance_ratio_.sum())\n",
    "    score1_logit_reduced, time1_logit_reduced = eval_clf(clf_logit, desc_reduced, 'empo_1')\n",
    "    score2_logit_reduced, time2_logit_reduced = eval_clf(clf_logit, desc_reduced, 'empo_2')\n",
    "    score3_logit_reduced, time3_logit_reduced = eval_clf(clf_logit, desc_reduced, 'empo_3')\n",
    "    score_cv.append([np.mean(score1_logit_reduced), np.mean(score2_logit_reduced), np.mean(score3_logit_reduced)])\n",
    "    time_cv.append([time1_logit_reduced,time2_logit_reduced,time3_logit_reduced])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_cv_empo1 = []\n",
    "time_cv_empo2 = []\n",
    "time_cv_empo3 = []\n",
    "for i in range(0,5):\n",
    "    time_cv_empo1.append(time_cv[i][0])\n",
    "    time_cv_empo2.append(time_cv[i][1])\n",
    "    time_cv_empo3.append(time_cv[i][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_cv_empo1 = []\n",
    "score_cv_empo2 = []\n",
    "score_cv_empo3 = []\n",
    "for i in range(0,5):\n",
    "    score_cv_empo1.append(score_cv[i][0])\n",
    "    score_cv_empo2.append(score_cv[i][1])\n",
    "    score_cv_empo3.append(score_cv[i][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = [100,200,500,1000,2000]\n",
    "plt.plot(n_components, score_cv_empo1, label = 'score cv empo_1')\n",
    "plt.plot(n_components, score_cv_empo2, label = 'score cv empo_2')\n",
    "plt.plot(n_components, score_cv_empo3, label = 'score cv empo_3')\n",
    "plt.xlabel('Number of features')\n",
    "plt.ylabel('Cross validation score')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(n_components, explained_variance_ratio , label = 'Explained variance')\n",
    "plt.xlabel('Number of features')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(n_components, time_cv_empo1, label = 'time cv empo_1')\n",
    "plt.plot(n_components, time_cv_empo2, label = 'time cv empo_2')\n",
    "plt.plot(n_components, time_cv_empo3, label = 'time cv empo_3')\n",
    "plt.xlabel('Number of features')\n",
    "plt.ylabel('Cross validation computational time')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD with n = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "start_time = time.time()\n",
    "svd500 = TruncatedSVD(n_components=500)\n",
    "svd500.fit(desc)\n",
    "print(\"svd with 500 components takes:\" , (time.time() - start_time), \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd500.explained_variance_ratio_.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_reduced500 = svd500.transform(desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score1_logit_reduced500 = cross_validate_clf(clf_logit, desc_reduced500, envs['empo_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score2_logit_reduced500 = cross_validate_clf(clf_logit, desc_reduced500, envs['empo_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score3_logit_reduced500 = cross_validate_clf(clf_logit, desc_reduced500, envs['empo_3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score1_logit_reduced500.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduced scores \n",
    "scores_logit_reduced500 = []\n",
    "scores_logit_reduced500.append(score1_logit_reduced500.get('test_accuracy'))\n",
    "scores_logit_reduced500.append(score2_logit_reduced500.get('test_accuracy'))\n",
    "scores_logit_reduced500.append(score3_logit_reduced500.get('test_accuracy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# scores of the full dataset and the reduced one, logistic regression\n",
    "scores = pd.read_csv('scores.csv', index_col=0)\n",
    "scores_logit = []\n",
    "\n",
    "empo1 = scores.loc[scores['empo'] == 'empo_1']\n",
    "empo2 = scores.loc[scores['empo'] == 'empo_2']\n",
    "empo3 = scores.loc[scores['empo'] == 'empo_3']\n",
    "\n",
    "empo1_logit = empo1.loc[empo1['clf_name'] == 'logit']\n",
    "empo2_logit = empo2.loc[empo2['clf_name'] == 'logit']\n",
    "empo3_logit = empo3.loc[empo3['clf_name'] == 'logit']\n",
    "\n",
    "empo1_logit_desc = empo1_logit.loc[empo1_logit['features'] == 'desc']\n",
    "empo2_logit_desc = empo2_logit.loc[empo2_logit['features'] == 'desc']\n",
    "empo3_logit_desc = empo3_logit.loc[empo3_logit['features'] == 'desc']\n",
    "\n",
    "a = empo1_logit_desc['test_accuracy'].values[0]\n",
    "empo1_logit_desc_test_accuracy = [float(x) for x in a[1:-1].split()]\n",
    "a = empo2_logit_desc['test_accuracy'].values[0]\n",
    "empo2_logit_desc_test_accuracy = [float(x) for x in a[1:-1].split()]\n",
    "a = empo3_logit_desc['test_accuracy'].values[0]\n",
    "empo3_logit_desc_test_accuracy = [float(x) for x in a[1:-1].split()]\n",
    "\n",
    "scores_logit = [empo1_logit_desc_test_accuracy, empo2_logit_desc_test_accuracy, empo3_logit_desc_test_accuracy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression cv scores with non reduced and reduced variables (zoom)\n",
    "for i in range(0,3):\n",
    "    fig, ax = plt.subplots()\n",
    "    mean = np.mean(scores_logit[i])\n",
    "    mean_reduced = np.mean(scores_logit_reduced500[i])\n",
    "    plt.bar([1,2,3,4,5], scores_logit[i], width=0.2)\n",
    "    plt.bar([1,2,3,4,5], scores_logit_reduced500[i], width=0.2)\n",
    "    plt.plot(np.linspace(0.5,5.5,10), mean*np.ones(10), linestyle=\"dashed\")\n",
    "    plt.plot(np.linspace(0.5,5.5,10), mean_reduced*np.ones(10), linestyle=\"dashed\")\n",
    "#     plt.ylim(0.80, 1)\n",
    "    plt.title(f'5 folds cross validation scores using empo {i+1}, \\n non reduced (blue) and reduced (orange) dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time\n",
    "times_logit_reduced500 = []\n",
    "times_logit_reduced500.append(score1_logit_reduced500.get('fit_time'))\n",
    "times_logit_reduced500.append(score2_logit_reduced500.get('fit_time'))\n",
    "times_logit_reduced500.append(score3_logit_reduced500.get('fit_time'))\n",
    "\n",
    "a = empo1_logit_desc['fit_time'].values[0]\n",
    "empo1_logit_desc_time = [float(x) for x in a[1:-1].split()]\n",
    "a = empo2_logit_desc['fit_time'].values[0]\n",
    "empo2_logit_desc_time = [float(x) for x in a[1:-1].split()]\n",
    "a = empo3_logit_desc['fit_time'].values[0]\n",
    "empo3_logit_desc_time = [float(x) for x in a[1:-1].split()]\n",
    "\n",
    "times_logit = [empo1_logit_desc_time, empo2_logit_desc_time, empo3_logit_desc_time]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,3):\n",
    "    fig, ax = plt.subplots()\n",
    "    mean = np.mean(times_logit[i])\n",
    "    mean_reduced = np.mean(times_logit_reduced500[i])\n",
    "    plt.bar([1,2,3,4,5], times_logit[i], width=0.2)\n",
    "    plt.bar([1,2,3,4,5], times_logit_reduced500[i], width=0.2)\n",
    "    plt.plot(np.linspace(0.5,5.5,10), mean*np.ones(10), linestyle=\"dashed\")\n",
    "    plt.plot(np.linspace(0.5,5.5,10), mean_reduced*np.ones(10), linestyle=\"dashed\")\n",
    "    plt.title(f'5 folds cross validation compuatational time using empo {i+1}, \\n non reduced (blue) and reduced (orange) dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score1_rforest_reduced500 = cross_validate_clf(clf_rforest, desc_reduced500, envs['empo_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score2_rforest_reduced500 = cross_validate_clf(clf_rforest, desc_reduced500, envs['empo_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score3_rforest_reduced500 = cross_validate_clf(clf_rforest, desc_reduced500, envs['empo_3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduced scores \n",
    "scores_rforest_reduced500 = []\n",
    "scores_rforest_reduced500.append(score1_rforest_reduced500.get('test_accuracy'))\n",
    "scores_rforest_reduced500.append(score2_rforest_reduced500.get('test_accuracy'))\n",
    "scores_rforest_reduced500.append(score3_rforest_reduced500.get('test_accuracy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores of the full dataset and the reduced one, logistic regression\n",
    "scores = pd.read_csv('scores.csv', index_col=0)\n",
    "scores_rforest = []\n",
    "\n",
    "empo1 = scores.loc[scores['empo'] == 'empo_1']\n",
    "empo2 = scores.loc[scores['empo'] == 'empo_2']\n",
    "empo3 = scores.loc[scores['empo'] == 'empo_3']\n",
    "\n",
    "empo1_rforest = empo1.loc[empo1['clf_name'] == 'rforest']\n",
    "empo2_rforest = empo2.loc[empo2['clf_name'] == 'rforest']\n",
    "empo3_rforest = empo3.loc[empo3['clf_name'] == 'rforest']\n",
    "\n",
    "empo1_rforest_desc = empo1_rforest.loc[empo1_rforest['features'] == 'desc']\n",
    "empo2_rforest_desc = empo2_rforest.loc[empo2_rforest['features'] == 'desc']\n",
    "empo3_rforest_desc = empo3_rforest.loc[empo3_rforest['features'] == 'desc']\n",
    "\n",
    "a = empo1_rforest_desc['test_accuracy'].values[0]\n",
    "empo1_rforest_desc_test_accuracy = [float(x) for x in a[1:-1].split()]\n",
    "a = empo2_rforest_desc['test_accuracy'].values[0]\n",
    "empo2_rforest_desc_test_accuracy = [float(x) for x in a[1:-1].split()]\n",
    "a = empo3_rforest_desc['test_accuracy'].values[0]\n",
    "empo3_rforest_desc_test_accuracy = [float(x) for x in a[1:-1].split()]\n",
    "\n",
    "scores_rforest = [empo1_rforest_desc_test_accuracy, empo2_rforest_desc_test_accuracy, empo2_rforest_desc_test_accuracy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression cv scores with non reduced and reduced variables (zoom)\n",
    "for i in range(0,3):\n",
    "    fig, ax = plt.subplots()\n",
    "    mean = np.mean(scores_rforest[i])\n",
    "    mean_reduced = np.mean(scores_rforest_reduced500[i])\n",
    "    plt.bar([1,2,3,4,5], scores_rforest[i], width=0.2)\n",
    "    plt.bar([1,2,3,4,5], scores_rforest_reduced500[i], width=0.2)\n",
    "    plt.plot(np.linspace(0.5,5.5,10), mean*np.ones(10), linestyle=\"dashed\")\n",
    "    plt.plot(np.linspace(0.5,5.5,10), mean_reduced*np.ones(10), linestyle=\"dashed\")\n",
    "#     plt.ylim(0.80, 1)\n",
    "    plt.title(f'5 folds cross validation scores using empo {i+1}, \\n non reduced (blue) and reduced (orange) dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time\n",
    "times_rforest_reduced500 = []\n",
    "times_rforest_reduced500.append(score1_rforest_reduced500.get('fit_time'))\n",
    "times_rforest_reduced500.append(score2_rforest_reduced500.get('fit_time'))\n",
    "times_rforest_reduced500.append(score3_rforest_reduced500.get('fit_time'))\n",
    "\n",
    "a = empo1_rforest_desc['fit_time'].values[0]\n",
    "empo1_rforest_desc_time = [float(x) for x in a[1:-1].split()]\n",
    "a = empo2_rforest_desc['fit_time'].values[0]\n",
    "empo2_rforest_desc_time = [float(x) for x in a[1:-1].split()]\n",
    "a = empo3_rforest_desc['fit_time'].values[0]\n",
    "empo3_rforest_desc_time = [float(x) for x in a[1:-1].split()]\n",
    "\n",
    "times_rforest = [empo1_rforest_desc_time, empo2_rforest_desc_time, empo3_rforest_desc_time]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,3):\n",
    "    fig, ax = plt.subplots()\n",
    "    mean = np.mean(times_rforest[i])\n",
    "    mean_reduced = np.mean(times_rforest_reduced500[i])\n",
    "    plt.bar([1,2,3,4,5], times_rforest[i], width=0.2)\n",
    "    plt.bar([1,2,3,4,5], times_rforest_reduced500[i], width=0.2)\n",
    "    plt.plot(np.linspace(0.5,5.5,10), mean*np.ones(10), linestyle=\"dashed\")\n",
    "    plt.plot(np.linspace(0.5,5.5,10), mean_reduced*np.ones(10), linestyle=\"dashed\")\n",
    "    plt.title(f'5 folds cross validation compuatational time using empo {i+1}, \\n non reduced (blue) and reduced (orange) dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biological interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.read_csv('scores.csv', index_col=0)\n",
    "scores.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores plots Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empo1_scores = scores.loc[scores['empo'] == 'empo_1']\n",
    "empo2_scores = scores.loc[scores['empo'] == 'empo_2']\n",
    "empo3_scores = scores.loc[scores['empo'] == 'empo_3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the logistic regression rows\n",
    "empo1_scores_logit = empo1_scores.loc[scores['clf_name'] == 'logit']\n",
    "empo2_scores_logit = empo2_scores.loc[scores['clf_name'] == 'logit']\n",
    "empo3_scores_logit = empo3_scores.loc[scores['clf_name'] == 'logit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score time\n",
    "plt.figure(figsize=(10,5))\n",
    "X = empo1_scores_logit['features'].iloc[1:-1]\n",
    "X_axis = np.arange(len(X))\n",
    "width=0.2\n",
    "\n",
    "plt.bar(X_axis - width, empo1_scores_logit['fit_time_median'].iloc[1:-1] / empo1_scores_logit['fit_time_median'].iloc[-1], width, label = 'empo_1')\n",
    "plt.bar(X_axis, empo2_scores_logit['fit_time_median'].iloc[1:-1] / empo2_scores_logit['fit_time_median'].iloc[-1], width, label = 'empo_2')\n",
    "plt.bar(X_axis + width, empo3_scores_logit['fit_time_median'].iloc[1:-1] / empo3_scores_logit['fit_time_median'].iloc[-1], width ,label = 'empo_3')\n",
    "plt.xticks(X_axis, X)\n",
    "\n",
    "plt.legend()\n",
    "plt.ylabel('Computational fit time mean')\n",
    "plt.title('Computational fit time mean using the taxonomies of the bacteria \\n with Logistic regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test accuracy\n",
    "plt.figure(figsize=(10,5))\n",
    "X = empo1_scores_logit['features'][1:]\n",
    "X_axis = np.arange(len(X))\n",
    "width=0.2\n",
    "\n",
    "plt.bar(X_axis - width, empo1_scores_logit['test_accuracy_median'][1:], width, label = 'empo_1')\n",
    "plt.bar(X_axis, empo2_scores_logit['test_accuracy_median'][1:], width, label = 'empo_2')\n",
    "plt.bar(X_axis + width, empo3_scores_logit['test_accuracy_median'][1:], width ,label = 'empo_3')\n",
    "plt.xticks(X_axis, X)\n",
    "\n",
    "plt.legend()\n",
    "plt.ylim([0.5,1])\n",
    "plt.ylabel('Test accuracy mean')\n",
    "plt.title('Test accuracy mean using the taxonomies of the bacteria \\n with Logistic regression (zoom on [0.5,1])')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1 score \n",
    "plt.figure(figsize=(10,5))\n",
    "X = empo1_scores_logit['features'][1:]\n",
    "X_axis = np.arange(len(X))\n",
    "width=0.2\n",
    "\n",
    "plt.bar(X_axis - width, empo1_scores_logit['test_f1_weighted_median'][1:], width, label = 'empo_1')\n",
    "plt.bar(X_axis, empo2_scores_logit['test_f1_weighted_median'][1:], width, label = 'empo_2')\n",
    "plt.bar(X_axis + width, empo3_scores_logit['test_f1_weighted_median'][1:], width ,label = 'empo_3')\n",
    "plt.xticks(X_axis, X)\n",
    "\n",
    "plt.legend()\n",
    "plt.ylim([0.5,1])\n",
    "plt.ylabel('F1 score mean')\n",
    "plt.title('F1 score mean mean using the taxonomies of the bacteria \\n with Logistic regression (zoom on [0.5,1])')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores plots Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the random forest rows\n",
    "empo1_scores_rforest = empo1_scores.loc[scores['clf_name'] == 'rforest']\n",
    "empo2_scores_rforest = empo2_scores.loc[scores['clf_name'] == 'rforest']\n",
    "empo3_scores_rforest = empo3_scores.loc[scores['clf_name'] == 'rforest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score time\n",
    "plt.figure(figsize=(10,5))\n",
    "X = empo1_scores_rforest['features'][1:]\n",
    "X_axis = np.arange(len(X))\n",
    "width=0.2\n",
    "\n",
    "plt.bar(X_axis - width, empo1_scores_rforest['fit_time_median'][1:], width, label = 'empo_1')\n",
    "plt.bar(X_axis, empo2_scores_rforest['fit_time_median'][1:], width, label = 'empo_2')\n",
    "plt.bar(X_axis + width, empo3_scores_rforest['fit_time_median'][1:], width ,label = 'empo_3')\n",
    "plt.xticks(X_axis, X)\n",
    "\n",
    "plt.legend()\n",
    "plt.ylabel('Computational fit time mean')\n",
    "plt.title('Computational fit time mean using the taxonomies of the bacteria \\n with Random Forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test accuracy\n",
    "plt.figure(figsize=(10,5))\n",
    "X = empo1_scores_rforest['features'][1:]\n",
    "X_axis = np.arange(len(X))\n",
    "width=0.2\n",
    "\n",
    "plt.bar(X_axis - width, empo1_scores_rforest['test_accuracy_median'][1:], width, label = 'empo_1')\n",
    "plt.bar(X_axis, empo2_scores_rforest['test_accuracy_median'][1:], width, label = 'empo_2')\n",
    "plt.bar(X_axis + width, empo3_scores_rforest['test_accuracy_median'][1:], width ,label = 'empo_3')\n",
    "plt.xticks(X_axis, X)\n",
    "\n",
    "plt.legend()\n",
    "plt.ylim([0.5,1])\n",
    "plt.ylabel('Test accuracy mean')\n",
    "plt.title('Test accuracy mean using the taxonomies of the bacteria \\n with Random Forest (zoom on [0.5,1])')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1 score \n",
    "plt.figure(figsize=(10,5))\n",
    "X = empo1_scores_rforest['features'][1:]\n",
    "X_axis = np.arange(len(X))\n",
    "width=0.2\n",
    "\n",
    "plt.bar(X_axis - width, empo1_scores_rforest['test_f1_weighted_median'][1:], width, label = 'empo_1')\n",
    "plt.bar(X_axis, empo2_scores_rforest['test_f1_weighted_median'][1:], width, label = 'empo_2')\n",
    "plt.bar(X_axis + width, empo3_scores_rforest['test_f1_weighted_median'][1:], width ,label = 'empo_3')\n",
    "plt.xticks(X_axis, X)\n",
    "\n",
    "plt.legend()\n",
    "plt.ylim([0.5,1])\n",
    "plt.ylabel('F1 score mean')\n",
    "plt.title('F1 score mean mean using the taxonomies of the bacteria \\n with Random Forest (zoom on [0.5,1])')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge_desc = sps.load_npz('challenge_descriptors_sparse.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge_envs = []\n",
    "for empo_name in empo_names:\n",
    "    clf_rforest.fit(desc, envs[empo_name])\n",
    "    \n",
    "    challenge_envs.append(clf_rforest.predict(challenge_desc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge_envs = np.array(challenge_envs)\n",
    "challenge_envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('predictions.csv', 'w') as f:\n",
    "    print(',' + ','.join(empo_names), file=f)\n",
    "    for sample_idx, line in enumerate(challenge_envs.T):\n",
    "        labels = [empo_index_to_label[empo_idx][cat_idx] for empo_idx, cat_idx in enumerate(line)]\n",
    "        \n",
    "        print(f'challenge_{sample_idx},' + ','.join(labels), file=f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
